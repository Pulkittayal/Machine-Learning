import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

path = 'https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DA0101EN/module_5_auto.csv'
df=pd.read_csv(path)

df=df._get_numeric_data()
df.head()

y_data=df["price"]
x_data=df.drop("price",axis=1)

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x_data,y_data,test_size=0.15,random_state=1)
print("The training data is:",x_train.shape[0])
print("The test data is:",x_test.shape[0])

from sklearn.linear_model import LinearRegression
lm=LinearRegression()
lm.fit(x_train[["horsepower"]],y_train)
lm.score(x_train[["horsepower"]],y_train)
lm.score(x_test[["horsepower"]],y_test)

#Sometimes you do not have sufficient testing data; as a result, you may want to perform Cross-validation. 
#Let's go over several methods that you can use for Cross-validation.
from sklearn.model_selection import cross_val_score
Rcross=cross_val_score(lm,x_data[["horsepower"]],y_data,cv=4)
print("The mean is:",Rcross.mean(),"and the standard deviation is:",Rcross.std())

#We can use negative squared error as a score by setting the parameter 'scoring' metric to 'neg_mean_squared_error'
from sklearn.model_selection import cross_val_predict
yhat=cross_val_predict(lm,x_data[["horsepower"]],y_data,cv=4)
yhat[0:5]

#Overfitting ,Underfitting and Model selection
lr=LinearRegression()
lr.fit(x_train[["horsepower",'curb-weight', 'engine-size', 'highway-mpg']],y_train)

#Prediction using Training data
yhat_train = lr.predict(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])
yhat_train[0:5]
yhat_test=lr.predict(x_test[["horsepower","curb-weight","engine-size","highway-mpg"]])
yhat_test[0:5]

# Overfitting
#Overfitting occurs when the model fits the noise, not the underlying process. Therefore when testing your model using the test-set, your model does not perform as well as it is modelling noise, not the underlying process that generated the relationship. Let's create a degree 5 polynomial model.

x_train,x_test,y_train,y_test=train_test_split(x_data,y_data,test_size=0.45,random_state=0)
from sklearn.preprocessing import PolynomialFeatures
pr=PolynomialFeatures(degree=5)
x_train_pr=pr.fit_transform(x_train[["horsepower"]])
x_test_pr=pr.fit_transform(x_test[["horsepower"]])
pr

poly=LinearRegression()
poly.fit(x_train_pr,y_train)yhat=poly.predict(x_test_pr)
yhat[0:5]

yhat=poly.predict(x_train_pr)
yhat[0:5]
print("the predicted values are:",yhat[0:4])
print("Actual value is:",y_test[0:4].values)

poly.score(x_train_pr,y_train)
poly.score(x_test_pr,y_test)
#We see the R^2 for the training data is 0.5567 while the R^2 on the test data was -29.87. 
#The lower the R^2, the worse the model, a Negative R^2 is a sign of overfitting.

Rsqu_test=[]
order=[1,2,3,4]
for n in order:
    pr=PolynomialFeatures(degree=n)
    x_train_pr=pr.fit_transform(x_train[["horsepower"]])
    x_test_pr=pr.fit_transform(x_test[["horsepower"]])
    lr=LinearRegression()
    lr.fit(x_train_pr,y_train)
    Rsqu_test.append(lr.score(x_test_pr,y_test))

    
plt.plot(order,Rsqu_test)
plt.xlabel("order")
plt.ylabel("Rsqu_test")
plt.title("order vs Rsqu_test")

def f(order, test_data):
    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_data, random_state=0)
    pr = PolynomialFeatures(degree=order)
    x_train_pr = pr.fit_transform(x_train[['horsepower']])
    x_test_pr = pr.fit_transform(x_test[['horsepower']])
    poly = LinearRegression()
    poly.fit(x_train_pr,y_train)
    PollyPlot(x_train[['horsepower']], x_test[['horsepower']], y_train,y_test, poly, pr)
    
    
# Ridge Regression
#our test data will be used as validation data.

pr=PolynomialFeatures(degree=2)
x_train_pr=pr.fit_transform(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])
x_test_pr=pr.fit_transform(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])

from sklearn.linear_model import Ridge
RidgeModel=Ridge(alpha=0.1)
RidgeModel.fit(x_train_pr,y_train)
yhat=RidgeModel.predict(x_test_pr)
yhat[0:5]

Rsqu_test=[]
Rsqu_train=[]
alpha=10* np.array(range(0,1000))

for n in alpha:
    RidgeModel=Ridge(alpha=n)
    RidgeModel.fit(x_train_pr,y_train)
    Rsqu_train.append(RidgeModel.score(x_train_pr,y_train))
    Rsqu_test.append(RidgeModel.score(x_test_pr,y_test))

plt.plot(alpha,Rsqu_train,"r",label="training data")
plt.plot(alpha,Rsqu_test,label="testing data")
plt.xlabel("alpha")
plt.ylabel("R^2")
plt.title("alpha vs R^2")


# grid Ssearch 
#If  we want to find the best value of alpha

from sklearn.model_selection import GridSearchCV
para1=[{"alpha":[0.001,0.1,1, 10, 100, 1000, 10000, 100000, 100000]}]
para1
RR=Ridge()
RR
Grid=GridSearchCV(RR,para1,cv=4)
Grid.fit(x_data[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']],y_data)

#The object finds the best parameter values on the validation data. We can obtain the estimator with the best parameters and assign it to the variable BestRR as follows:
BestRR=Grid.best_estimator_
BestRR

BestRR.score(x_data[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']],y_data)


